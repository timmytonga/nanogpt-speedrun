{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "from train_gpt import GPT\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files: str = \"data/fineweb10B/fineweb_train_*.bin\" # input .bin to train on\n",
    "    val_files: str = \"data/fineweb10B/fineweb_val_*.bin\" # input .bin to eval validation loss on\n",
    "    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    train_seq_len: int = 48*1024 # FlexAttention sequence length\n",
    "    val_seq_len: int = 4*64*1024 # FlexAttention sequence length for validation\n",
    "    # optimization\n",
    "    num_iterations: int = 1770 # number of iterations to run\n",
    "    cooldown_frac: float = 0.4 # fraction of training spent cooling down the learning rate\n",
    "    # architecture\n",
    "    vocab_size: int = 50257\n",
    "    # evaluation and logging\n",
    "    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint: bool = False\n",
    "    wandb: bool = False\n",
    "    # optimizer setup\n",
    "    lr: float = 0.05  # this is the default for muon\n",
    "    opt1: str = \"adam\"  # choices = ['adam', 'adamw_sn']\n",
    "    optimizer: str = \"muon\"  # choices = ['muon', 'adamw_sn', 'adamw_snsm']\n",
    "    beta1: float = 0.9  # momentum \n",
    "    beta2: float = 0.95  \n",
    "    use_momentum_sched: bool = False \n",
    "    muon_momentum_warmup: bool = True     \n",
    "    single_gpu: bool = True\n",
    "    # scheduler\n",
    "    scheduler: str = \"default\"  # choices: ['linear', 'default']. default is \"constant then decay\".\n",
    "    warmup: int = 0  # specify the number of warmup steps\n",
    "    final_rate: float = 0.1  # final lr decay fraction\n",
    "    # adamw_snsm args\n",
    "    rank: int = 256\n",
    "    update_proj_gap: int = 200\n",
    "    \n",
    "    \n",
    "\n",
    "args = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing model\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.12 MiB is free. Process 1307296 has 21.30 GiB memory in use. Including non-PyTorch memory, this process has 2.31 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 11.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstructing model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model: nn\u001b[38;5;241m.\u001b[39mModule \u001b[38;5;241m=\u001b[39m \u001b[43mGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_seq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mEmbedding):\n",
      "File \u001b[0;32m~/miniconda3/envs/speedrun/lib/python3.12/site-packages/torch/nn/modules/module.py:1065\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speedrun/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/speedrun/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/speedrun/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/speedrun/lib/python3.12/site-packages/torch/nn/modules/module.py:1003\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/speedrun/lib/python3.12/site-packages/torch/nn/modules/module.py:1065\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.12 MiB is free. Process 1307296 has 21.30 GiB memory in use. Including non-PyTorch memory, this process has 2.31 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 11.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(\"Constructing model\")\n",
    "model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,\n",
    "        max_seq_len=max(args.train_seq_len, args.val_seq_len))\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "\n",
    "print(\"Constructing optimizers\")\n",
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t0.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t0.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t0.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "1.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t1.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t1.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t1.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "2.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t2.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t2.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t2.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "3.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t3.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t3.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t3.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "4.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t4.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t4.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t4.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "5.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t5.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t5.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t5.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "6.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t6.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t6.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t6.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "\t7.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t7.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "8.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t8.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t8.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t8.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "9.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t9.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t9.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t9.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "10.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t10.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t10.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t10.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n",
      "11.attn.qkv_w: shape = torch.Size([3, 768, 768])\n",
      "\t11.attn.c_proj.weight: shape = torch.Size([768, 768])\n",
      "\t11.mlp.c_fc.weight: shape = torch.Size([3072, 768])\n",
      "\t11.mlp.c_proj.weight: shape = torch.Size([768, 3072])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.blocks.named_parameters():\n",
    "    if p.ndim == 2:\n",
    "        print(f\"\\t{n}: shape = {p.shape}\")\n",
    "    if p.ndim > 2:\n",
    "        print(f\"{n}: shape = {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speedrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
